# Local AI Assistant - Changli UI
![Python](https://img.shields.io/badge/Python-3.10-blue?logo=python)  ![Flask](https://img.shields.io/badge/Flask-2.3-lightgrey?logo=flask)  ![PySide6](https://img.shields.io/badge/PySide6-Qt-brightgreen?logo=qt)  ![Ollama](https://img.shields.io/badge/Ollama-AI-orange?logo=openai)  ![Status](https://img.shields.io/badge/Status-Development-red)  ![License](https://img.shields.io/badge/License-MIT-yellow)


---

üöß **Status:** This project is still in **development**, so expect bugs, incomplete features, & potential breaking changes. üöß

---

## üìñ Description
Local AI Assistant is an **offline/local AI chat application** featuring:
- **Backend** built with Flask ‚Üí API communication and chat storage.
- **Frontend (UI)** built with PySide6 (Qt) ‚Üí interactive chat interface.
- **Ollama Integration** ‚Üí run local AI models (default: `gemma3:4b`, now supports model selection).

This project is designed to run AI **fully locally**, with customizable identity, persona, profile, memory, and chat history.

---

## üìÇ Project Structure
```
local-ai-assistant/
‚îú‚îÄ app.py                    
‚îÇ
‚îú‚îÄ backend/
‚îÇ  ‚îú‚îÄ config.py      
‚îÇ  ‚îú‚îÄ core.py           
‚îÇ  ‚îú‚îÄ persona.py            
‚îÇ  ‚îú‚îÄ storage.py             
‚îÇ  ‚îú‚îÄ ollama_client.py       
‚îÇ  ‚îî‚îÄ routes.py              
‚îÇ
‚îú‚îÄ ui/
‚îÇ  ‚îú‚îÄ main.py               
‚îÇ  ‚îú‚îÄ chat_window.py         
‚îÇ  ‚îú‚îÄ client.py              
‚îÇ  ‚îú‚îÄ worker.py              
‚îÇ  ‚îî‚îÄ widgets/              
‚îÇ     ‚îú‚îÄ settings.py         
‚îÇ     ‚îú‚îÄ history.py          
‚îÇ     ‚îú‚îÄ bubbles.py          
‚îÇ     ‚îî‚îÄ identity.py        
‚îÇ
‚îú‚îÄ data/                    
‚îÇ  ‚îú‚îÄ chat_history.json
‚îÇ  ‚îú‚îÄ chat_sessions.json
‚îÇ  ‚îú‚îÄ ui_chat_config.json
‚îÇ  ‚îî‚îÄ config.json
‚îÇ
‚îî‚îÄ README.md                 
```

## üèó Architecture

![Architecture Diagram](assets/architecture_diagram.png)

---

## ‚ö° Features
- üñ• **Modern UI** using PySide6 (Qt)
- üìù **Chat History** ‚Üí rename, delete, or continue past sessions
- üé≠ **Custom Persona** ‚Üí change AI name, user name, and prompt
- üë§ **Profile System** ‚Üí add personal info (e.g. *What do you do*, *Anything else the AI should know*)
- üß† **Chat Memory** ‚Üí AI remembers up to **8 previous messages**
- üé® **Custom Background** ‚Üí solid color or custom image
- ‚öôÔ∏è **Flask Backend** with `/chat`, `/chats`, `/config` endpoints
- ü§ñ **Ollama Integration** ‚Üí run local AI models; default `gemma3:4b`, now supports **model selection**
- üìÇ All data & configs stored locally under `data/`

---

## üõ†Ô∏è Installation & Setup

### 1. Clone Repository
```bash
git clone https://github.com/rillToMe/local-ai-assistant.git
cd local-ai-assistant
```

### 2. Create Virtual Environment (recommended)
```bash
python -m venv venv
source venv/bin/activate     # Linux/Mac
venv\Scripts\activate        # Windows
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

**Main dependencies:**
- `flask`
- `flask-cors`
- `requests`
- `PySide6`

> ‚ö†Ô∏è **Note**: Make sure you have installed **[Ollama](https://ollama.ai/)** and the required models (`gemma3:4b` by default). Other models can be selected if installed.

### 4. Run the App
```bash
python app.py
```

- Flask backend will start on `http://127.0.0.1:5000`
- PySide6 UI will automatically open

---

## üéÆ Usage
- Click **Settings** ‚Üí change background, update **Identity & Prompt**
- Click **History** ‚Üí view, rename, or continue previous sessions
- Click **Profile** ‚Üí add info about yourself (used by AI in responses)
- Default AI identity is **Changli** (can be changed via settings)
- Choose available Ollama models via **Model Selection**

---

## üß© Model Recommendations  

[![Gemma Docs](https://img.shields.io/badge/Gemma-Docs-blue)](https://ai.google.dev/gemma/docs)
[![Qwen HF](https://img.shields.io/badge/Qwen-HuggingFace-orange)](https://huggingface.co/Qwen)
[![DeepSeek HF](https://img.shields.io/badge/DeepSeek-HuggingFace-green)](https://huggingface.co/deepseek-ai)
[![GPT-OSS](https://img.shields.io/badge/GPT--OSS-Research-purple)](https://openai.com/research/gpt-oss)
[![Ollama Models](https://img.shields.io/badge/Ollama-Models-black)](https://ollama.com/library)

Based on official benchmarks and community tests:  

| Model            | Min RAM (CPU-only)      | Approx. GPU VRAM (BF16 / 4-bit) | Notes |
|------------------|--------------------------|----------------------------------|-------|
| **gemma3:1b**    | ‚â•‚ÄØ2‚ÄØGB RAM               | ~1.5‚ÄØGB / ~0.9‚ÄØGB                | Lightweight - runs on old notebooks, but slow (~7‚Äì10‚ÄØtokens/sec) [getdeploying.com](https://getdeploying.com/guides/local-gemma3?utm_source=chatgpt.com), [windowscentral.com](https://www.windowscentral.com/artificial-intelligence/my-seven-year-old-mid-range-laptop-runs-local-ai?utm_source=chatgpt.com) |
| **qwen3:1.8b**   | ‚â•‚ÄØ2‚ÄØGB RAM               | ~2‚ÄØGB / ~1‚ÄØGB (est.)             | Slightly better reasoning - light enough for laptops |
| **gemma3:4b**    | ‚â•‚ÄØ4‚ÄØGB RAM               | ~6.4‚ÄØGB / ~3.4‚ÄØGB                | Recommended default - good speed & quality [getdeploying.com](https://getdeploying.com/guides/local-gemma3?utm_source=chatgpt.com), [ai.google.dev](https://ai.google.dev/gemma/docs/core?utm_source=chatgpt.com) |
| **qwen3:4b**     | ‚â•‚ÄØ4‚ÄØGB RAM               | ~6‚ÄØGB / ~3‚ÄØGB (est.)             | Balanced - strong chat & reasoning |
| **gemma3:12b**   | ‚â•‚ÄØ9‚ÄØGB RAM               | ~20‚ÄØGB / ~8.7‚ÄØGB                 | Requires strong GPU or high RA[getdeploying.com](https://getdeploying.com/guides/local-gemma3?utm_source=chatgpt.com), [ai.google.dev](https://ai.google.dev/gemma/docs/core?utm_source=chatgpt.com) |
| **qwen3:8b**     | ‚â•‚ÄØ9‚ÄØGB RAM               | ~18‚ÄØGB / ~8‚ÄØGB (est.)            | Good quality & context |
| **deepseek-r1:8b** | ‚â•‚ÄØ9‚ÄØGB RAM             | ~18‚ÄØGB / ~8‚ÄØGB (est.)            | Specialized reasoning |
| **gemma3:27b**   | ‚â•‚ÄØ18‚ÄØGB RAM              | ~46‚ÄØGB / ~21‚ÄØGB                  | Heavy - best on high-end GPUs or servers [getdeploying.com](https://getdeploying.com/guides/local-gemma3?utm_source=chatgpt.com), [ai.google.dev](https://ai.google.dev/gemma/docs/core?utm_source=chatgpt.com) |
| **gpt-oss:20b**  | ‚â•‚ÄØ32‚ÄØGB RAM              | ~40‚ÄØGB / ~20‚ÄØGB (est.)           | Large - better long context |
| **gpt-oss:120b** | ‚â•‚ÄØ128‚ÄØGB RAM / Multi-GPU | ~120‚ÄØGB+ / 60‚ÄØGB+ (est.)         | Experimental - extremely heavy compute requirement |

---

## üìå Roadmap / Todo
- [ ] Fix UI crash bugs
- [ ] Add multi-tab chat support
- [ ] Add chat export/import
- [ ] Optimize performance for long requests
- [ ] Expand model integration beyond Ollama

---

## ‚ö†Ô∏è Notes
- This is still in **early development**, expect frequent bugs and issues  
- UI/UX is minimal for now, focused on core functionality  
- Default persona is simple, but can be extended with custom prompts  
