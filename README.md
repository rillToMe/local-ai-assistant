# Local AI Assistant - Changli UI
![Python](https://img.shields.io/badge/Python-3.10-blue?logo=python)  ![Flask](https://img.shields.io/badge/Flask-2.3-lightgrey?logo=flask)  ![PySide6](https://img.shields.io/badge/PySide6-Qt-brightgreen?logo=qt)  ![Ollama](https://img.shields.io/badge/Ollama-AI-orange?logo=openai)  ![Status](https://img.shields.io/badge/Status-Development-red)  ![License](https://img.shields.io/badge/License-MIT-yellow)


---


[![Gemma Docs](https://img.shields.io/badge/Gemma-Docs-blue)](https://ai.google.dev/gemma/docs)
[![Qwen HF](https://img.shields.io/badge/Qwen-HuggingFace-orange)](https://huggingface.co/Qwen)
[![DeepSeek HF](https://img.shields.io/badge/DeepSeek-HuggingFace-green)](https://huggingface.co/deepseek-ai)
[![GPT-OSS](https://img.shields.io/badge/GPT--OSS-Research-purple)](https://openai.com/research/gpt-oss)
[![Ollama Models](https://img.shields.io/badge/Ollama-Models-black)](https://ollama.com/library)

---

ğŸš§ **Status:** This project is still in **development**, so expect bugs, incomplete features, & potential breaking changes. ğŸš§

---

## ğŸ“– Description
Local AI Assistant is an **offline/local AI chat application** featuring:
- **Backend** built with Flask â†’ API communication and chat storage.
- **Frontend (UI)** built with PySide6 (Qt) â†’ interactive chat interface.
- **Ollama Integration** â†’ run local AI models (default: `gemma3:4b`, now supports model selection).

This project is designed to run AI **fully locally**, with customizable identity, persona, profile, memory, and chat history.

---

## ğŸ“‚ Project Structure
```
local-ai-assistant/
â”œâ”€ app.py                    
â”‚
â”œâ”€ backend/
â”‚  â”œâ”€ config.py      
â”‚  â”œâ”€ core.py           
â”‚  â”œâ”€ persona.py            
â”‚  â”œâ”€ storage.py             
â”‚  â”œâ”€ ollama_client.py       
â”‚  â””â”€ routes.py              
â”‚
â”œâ”€ ui/
â”‚  â”œâ”€ main.py               
â”‚  â”œâ”€ chat_window.py         
â”‚  â”œâ”€ client.py              
â”‚  â”œâ”€ worker.py              
â”‚  â””â”€ widgets/              
â”‚     â”œâ”€ settings.py         
â”‚     â”œâ”€ history.py          
â”‚     â”œâ”€ bubbles.py          
â”‚     â””â”€ identity.py        
â”‚
â”œâ”€ data/                    
â”‚  â”œâ”€ chat_history.json
â”‚  â”œâ”€ chat_sessions.json
â”‚  â”œâ”€ ui_chat_config.json
â”‚  â””â”€ config.json
â”‚
â””â”€ README.md                 
```

## ğŸ— Architecture

![Architecture Diagram](asset/architecture_diagram.png)

---

## âš¡ Features
- ğŸ–¥ **Modern UI** using PySide6 (Qt)
- ğŸ“ **Chat History** â†’ rename, delete, or continue past sessions
- ğŸ­ **Custom Persona** â†’ change AI name, user name, and prompt
- ğŸ‘¤ **Profile System** â†’ add personal info (e.g. *What do you do*, *Anything else the AI should know*)
- ğŸ§  **Chat Memory** â†’ AI remembers up to **8 previous messages**
- ğŸ¨ **Custom Background** â†’ solid color or custom image
- âš™ï¸ **Flask Backend** with `/chat`, `/chats`, `/config` endpoints
- ğŸ¤– **Ollama Integration** â†’ run local AI models; default `gemma3:4b`, now supports **model selection**
- ğŸ“‚ All data & configs stored locally under `data/`

---

## ğŸ› ï¸ Installation & Setup

### 1. Clone Repository
```bash
git clone https://github.com/rillToMe/local-ai-assistant.git
cd local-ai-assistant
```

### 2. Create Virtual Environment (recommended)
```bash
python -m venv venv
source venv/bin/activate     # Linux/Mac
venv\Scripts\activate        # Windows
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

**Main dependencies:**
- `flask`
- `flask-cors`
- `requests`
- `PySide6`

> âš ï¸ **Note**: Make sure you have installed **[Ollama](https://ollama.ai/)** and the required models (`gemma3:4b` by default). Other models can be selected if installed.

### 4. Run the App
```bash
python app.py
```

- Flask backend will start on `http://127.0.0.1:5000`
- PySide6 UI will automatically open

---

## ğŸ® Usage
- Click **Settings** â†’ change background, update **Identity & Prompt**
- Click **History** â†’ view, rename, or continue previous sessions
- Click **Profile** â†’ add info about yourself (used by AI in responses)
- Default AI identity is **Changli** (can be changed via settings)
- Choose available Ollama models via **Model Selection**

---

## ğŸ§© Model Recommendations  

Based on official benchmarks and community tests:  

| Model            | Min RAM (CPU-only)      | Approx. GPU VRAM (BF16 / 4-bit) | Notes |
|------------------|--------------------------|----------------------------------|-------|
| **gemma3:1b**    | â‰¥â€¯2â€¯GB RAM               | ~1.5â€¯GB / ~0.9â€¯GB                | Lightweight - runs on old notebooks, but slow (~7â€“10â€¯tokens/sec) [getdeploying.com](https://getdeploying.com/guides/local-gemma3?utm_source=chatgpt.com), [windowscentral.com](https://www.windowscentral.com/artificial-intelligence/my-seven-year-old-mid-range-laptop-runs-local-ai?utm_source=chatgpt.com) |
| **qwen3:1.8b**   | â‰¥â€¯2â€¯GB RAM               | ~2â€¯GB / ~1â€¯GB (est.)             | Slightly better reasoning - light enough for laptops |
| **gemma3:4b**    | â‰¥â€¯4â€¯GB RAM               | ~6.4â€¯GB / ~3.4â€¯GB                | Recommended default - good speed & quality [getdeploying.com](https://getdeploying.com/guides/local-gemma3?utm_source=chatgpt.com), [ai.google.dev](https://ai.google.dev/gemma/docs/core?utm_source=chatgpt.com) |
| **qwen3:4b**     | â‰¥â€¯4â€¯GB RAM               | ~6â€¯GB / ~3â€¯GB (est.)             | Balanced - strong chat & reasoning |
| **gemma3:12b**   | â‰¥â€¯9â€¯GB RAM               | ~20â€¯GB / ~8.7â€¯GB                 | Requires strong GPU or high RA[getdeploying.com](https://getdeploying.com/guides/local-gemma3?utm_source=chatgpt.com), [ai.google.dev](https://ai.google.dev/gemma/docs/core?utm_source=chatgpt.com) |
| **qwen3:8b**     | â‰¥â€¯9â€¯GB RAM               | ~18â€¯GB / ~8â€¯GB (est.)            | Good quality & context |
| **deepseek-r1:8b** | â‰¥â€¯9â€¯GB RAM             | ~18â€¯GB / ~8â€¯GB (est.)            | Specialized reasoning |
| **gemma3:27b**   | â‰¥â€¯18â€¯GB RAM              | ~46â€¯GB / ~21â€¯GB                  | Heavy - best on high-end GPUs or servers [getdeploying.com](https://getdeploying.com/guides/local-gemma3?utm_source=chatgpt.com), [ai.google.dev](https://ai.google.dev/gemma/docs/core?utm_source=chatgpt.com) |
| **gpt-oss:20b**  | â‰¥â€¯32â€¯GB RAM              | ~40â€¯GB / ~20â€¯GB (est.)           | Large - better long context |
| **gpt-oss:120b** | â‰¥â€¯128â€¯GB RAM / Multi-GPU | ~120â€¯GB+ / 60â€¯GB+ (est.)         | Experimental - extremely heavy compute requirement |

---

## ğŸ“Œ Roadmap / Todo
- [ ] Fix UI crash bugs
- [ ] Add multi-tab chat support
- [ ] Add chat export/import
- [ ] Optimize performance for long requests
- [ ] Expand model integration beyond Ollama

---

## âš ï¸ Notes
- This is still in **early development**, expect frequent bugs and issues  
- UI/UX is minimal for now, focused on core functionality  
- Default persona is simple, but can be extended with custom prompts  
