# Local AI Assistant ‚Äî Changli UI

üöß **Status:** This project is still in **development**, so expect bugs, incomplete features, and potential breaking changes. üöß

---

## üìñ Description
Local AI Assistant is an **offline/local AI chat application** featuring:
- **Backend** built with Flask ‚Üí API communication and chat storage.
- **Frontend (UI)** built with PySide6 (Qt) ‚Üí interactive chat interface.
- **Ollama Integration** ‚Üí run local AI models (default: `gemma3:4b`, now supports model selection).

This project is designed to run AI **fully locally**, with customizable identity, persona, profile, memory, and chat history.

---

## üìÇ Project Structure
```
local-ai-assistant/
‚îú‚îÄ app.py                    
‚îÇ
‚îú‚îÄ backend/
‚îÇ  ‚îú‚îÄ config.py      
‚îÇ  ‚îú‚îÄ core.py           
‚îÇ  ‚îú‚îÄ persona.py            
‚îÇ  ‚îú‚îÄ storage.py             
‚îÇ  ‚îú‚îÄ ollama_client.py       
‚îÇ  ‚îî‚îÄ routes.py              
‚îÇ
‚îú‚îÄ ui/
‚îÇ  ‚îú‚îÄ main.py               
‚îÇ  ‚îú‚îÄ chat_window.py         
‚îÇ  ‚îú‚îÄ client.py              
‚îÇ  ‚îú‚îÄ worker.py              
‚îÇ  ‚îî‚îÄ widgets/              
‚îÇ     ‚îú‚îÄ settings.py         
‚îÇ     ‚îú‚îÄ history.py          
‚îÇ     ‚îú‚îÄ bubbles.py          
‚îÇ     ‚îî‚îÄ identity.py        
‚îÇ
‚îú‚îÄ data/                    
‚îÇ  ‚îú‚îÄ chat_history.json
‚îÇ  ‚îú‚îÄ chat_sessions.json
‚îÇ  ‚îú‚îÄ ui_chat_config.json
‚îÇ  ‚îî‚îÄ config.json
‚îÇ
‚îî‚îÄ README.md                 
```

---

## ‚ö° Features
- üñ• **Modern UI** using PySide6 (Qt)
- üìù **Chat History** ‚Üí rename, delete, or continue past sessions
- üé≠ **Custom Persona** ‚Üí change AI name, user name, and prompt
- üë§ **Profile System** ‚Üí add personal info (e.g. *What do you do*, *Anything else the AI should know*)
- üß† **Chat Memory** ‚Üí AI remembers up to **8 previous messages**
- üé® **Custom Background** ‚Üí solid color or custom image
- ‚öôÔ∏è **Flask Backend** with `/chat`, `/chats`, `/config` endpoints
- ü§ñ **Ollama Integration** ‚Üí run local AI models; default `gemma3:4b`, now supports **model selection**
- üìÇ All data & configs stored locally under `data/`

---

## üõ†Ô∏è Installation & Setup

### 1. Clone Repository
```bash
git clone https://github.com/rillToMe/local-ai-assistant.git
cd local-ai-assistant
```

### 2. Create Virtual Environment (recommended)
```bash
python -m venv venv
source venv/bin/activate     # Linux/Mac
venv\Scripts\activate        # Windows
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

**Main dependencies:**
- `flask`
- `flask-cors`
- `requests`
- `PySide6`

> ‚ö†Ô∏è **Note**: Make sure you have installed **[Ollama](https://ollama.ai/)** and the required models (`gemma3:4b` by default). Other models can be selected if installed.

### 4. Run the App
```bash
python app.py
```

- Flask backend will start on `http://127.0.0.1:5000`
- PySide6 UI will automatically open

---

## üéÆ Usage
- Click **Settings** ‚Üí change background, update **Identity & Prompt**
- Click **History** ‚Üí view, rename, or continue previous sessions
- Click **Profile** ‚Üí add info about yourself (used by AI in responses)
- Default AI identity is **Changli** (can be changed via settings)
- Choose available Ollama models via **Model Selection**

---

## üìå Roadmap / Todo
- [ ] Fix UI crash bugs
- [ ] Add multi-tab chat support
- [ ] Add chat export/import
- [ ] Optimize performance for long requests
- [ ] Expand model integration beyond Ollama

---

## ‚ö†Ô∏è Notes
- This is still in **early development**, expect frequent bugs and issues  
- UI/UX is minimal for now, focused on core functionality  
- Default persona is simple, but can be extended with custom prompts  
